# 向量转化

请使用python代码说明，将一段中文通过embedding模型转换后，存入milvus数据库 embedding模型选用：text2vec-base-chinese

****

要将中文文本通过`text2vec-base-chinese`模型转换为embedding向量并存储到Milvus数据库中，你需要执行几个步骤。以下是一个简化的Python代码示例，说明如何完成这个过程：

1. 安装必要的库（如`transformers`和`pymilvus`）。
2. 加载`text2vec-base-chinese`模型。
3. 将中文文本转换为embedding向量。
4. 连接到Milvus数据库。
5. 在Milvus中创建集合（collection）并插入embedding向量。

首先，确保你已经安装了必要的库：

```python
from transformers import AutoTokenizer, AutoModel  
from pymilvus import connections, Collection, FieldSchema, CollectionMapping  
import numpy as np  
  
# 1. 加载模型  
tokenizer = AutoTokenizer.from_pretrained("hfl/text2vec-base-chinese")  
model = AutoModel.from_pretrained("hfl/text2vec-base-chinese")  
  
# 假设的中文文本  
text = "你好，世界！"  
  
# 2. 将中文文本转换为embedding向量  
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)  
with torch.no_grad():  
    outputs = model(**inputs)  
    last_hidden_states = outputs.last_hidden_state   # 取最后一个隐藏状态作为embedding 
    embedding = last_hidden_states.mean(dim=1).squeeze().numpy()  # 取平均向量作为整个句子的embedding  
# embedding_vector = last_hidden_states[0][0].numpy()  # 取第一个token（通常是[CLS]）的embedding
  
# 3. 连接到Milvus数据库（需要提供你的连接参数）  
connections.connect("default", host='localhost', port='19530')  
  
# 4. 在Milvus中创建集合  
if not Collection.has_collection("text_embeddings"):  
    field1 = FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=embedding.shape[-1])  
    collection_mapping = CollectionMapping(  
        fields=[field1],  
        default_vector_dim=embedding.shape[-1]  
    )  
    Collection.create_collection("text_embeddings", collection_mapping)  
  
# 5. 插入embedding向量到Milvus集合中  
collection = Collection("text_embeddings")  
collection.insert([[embedding]])  
  
# 断开Milvus连接（可选）  
connections.disconnect("default")
```

**注意**：

- 请确保你的Milvus服务正在运行，并且你可以使用提供的连接参数连接到它。
- 在上面的代码中，我使用了`torch.no_grad()`来确保在推理过程中不计算梯度，从而加速计算。但是，你需要确保已经导入了`torch`库（尽管在上面的代码中没有明确导入）。
- 我假设你希望使用句子的平均embedding作为整个句子的表示。但你可以根据需要选择其他策略（如CLS token的embedding）。
- 请确保你已经为Milvus数据库设置了正确的权限和认证（如果需要的话）。
- 上述代码是一个简化的示例，用于说明整个过程。在实际应用中，你可能需要处理多个文本并批量插入embedding向量。
