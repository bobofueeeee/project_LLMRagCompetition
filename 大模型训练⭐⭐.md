

# 1. 训练环境搭建

## 1.1 windows cuda环境安装

```shell
# 参考笔记
地址：https://blog.csdn.net/weixin_46528483/article/details/136880436
地址：https://blog.csdn.net/qq_45921549/article/details/138758637
cuda版本更新：https://blog.csdn.net/yxn4065/article/details/139469418
参考⭐：https://blog.csdn.net/sslfk/article/details/127857073
cudatoolkit下载地址：https://developer.nvidia.com/cuda-toolkit-archive
```

```shell
# 1. 创建coda环境
conda create -n torch_gpu4 python=3.12  cudatoolkit=12.1.0
# 2. pip安装相关的库(需要注意的是，torch版本和cudatoolkit和显卡驱动的正确对应)
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113
pip3 install torch==2.3.0+cu121 --index-url https://download.pytorch.org/whl/cu121
```





# 2. 模型训练思路

参考⭐⭐：https://blog.csdn.net/SoulmateY/article/details/139564504

transformer⭐⭐：https://github.com/huggingface/transformers

aplaca⭐⭐：https://github.com/search?q=alpaca&type=repositories



## 2.1 数据预处理

参考：https://gitee.com/fubob/deepLearning_frameWork_learning/blob/master/torch_learning/torch_demo.py#

跑一下文件，就可以清楚了解预处理的过程

1. 进行tokenizer，分词，加入了['CLS']，['SEP']标记
2. 再找到需要mask的起始和结束位置，并替换['mask']
3. 再通过convert_tokens_to_ids转化为vocab词汇表中的id
4. 再通过torch.tensor，转化为torch的向量
5. 构建input_ids，labels的torch向量数据集

需要注意的是：input_ids是带有mask的，而labels是不带有mask的



## 2.2 优化器的选择

```python
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
```

```
大模型的训练有哪些常用的优化器optimizer，如何进行选择
-------------------------------------------------------------------
大模型的训练过程中，常用的优化器（optimizer）主要包括以下几种：

常用的优化器
SGD（Stochastic Gradient Descent）：
原理：SGD是一种基本的优化算法，它在每次迭代中随机选择一个样本进行梯度计算和参数更新。
特点：使用固定的学习率，通常需要更多的迭代才能收敛，但在一些情况下也可以取得很好的效果，简单易于实现。然而，在非凸优化问题中可能会出现收敛速度慢的问题。
RMSprop（Root Mean Square Propagation）：
原理：RMSprop是自适应学习率的一种方法，通过维护梯度平方的移动平均来调整学习率，以便更好地适应不同特征的梯度。
特点：能够自适应地调整学习率，适用于解决一些SGD难以处理的问题。
Adagrad（Adaptive Gradient）：
原理：Adagrad是一种自适应学习率的优化器，它根据参数的历史梯度进行学习率调整。
特点：适用于稀疏数据集，在训练初期对稀疏特征有较大的学习率，但可能会在训练后期由于学习率过小导致收敛速度变慢。
Adadelta：
原理：Adadelta是Adagrad的改进版本，通过引入梯度平方的衰减平均来解决Adagrad学习率过早衰减的问题。
特点：不需要手动设置学习率，并且在训练过程中可以自适应地调整学习率。
Adam（Adaptive Moment Estimation）：
原理：Adam结合了自适应学习率和动量的概念，在训练过程中自适应地调整学习率，并利用动量来加速梯度更新。
特点：在很多NLP任务中表现良好，是目前广泛使用的优化器之一。
Adamax：
原理：Adamax是Adam的变体，它使用了无穷范数（infinity norm）来对梯度进行归一化。
特点：在某些情况下可能提供更好的性能，但使用不如Adam普遍。
AdamW：
原理：AdamW是Adam的一种变体，引入了权重衰减（weight decay）的概念。
特点：权重衰减可以有效防止模型过拟合，使得AdamW在某些情况下比Adam更具优势。
Nadam（Nesterov-accelerated Adaptive Moment Estimation）：
原理：Nadam是Adam与Nesterov动量法的结合，在Adam的基础上加入了Nesterov动量的修正项。
特点：结合了Nesterov动量法的优点，可能在一些情况下提供更快的收敛速度。
如何进行选择
了解优化器的原理和特点：
深入理解每种优化器的原理、优缺点以及适用场景，这是选择优化器的基础。
考虑问题的类型：
不同的问题类型（如分类、回归、目标检测等）可能会对优化器的性能有不同的要求。例如，对于分类问题，Adam等自适应学习率优化器通常表现良好。
实验和评估：
在实际训练中，可以尝试使用不同的优化器并进行比较。通过在相同的数据集和模型上使用不同的优化器进行训练，可以比较它们在模型性能、收敛速度等方面的表现。
考虑学习率调度：
一些优化器可能对学习率的变化更敏感，因此需要结合合适的学习率调度策略。合理的学习率调度可以进一步提高优化器的性能。
查阅相关文献和社区讨论：
查阅相关的研究论文、博客文章以及社区讨论，了解在类似问题上的优化器选择和实践经验。这有助于更全面地了解各种优化器的性能和适用场景。
综上所述，选择大模型训练中的优化器需要结合具体的问题、模型和数据集来进行综合考虑。通过多次尝试和评估，可以找到最适合特定情况的优化器。
```



## 2.3 损失函数的选择

```
大语言模型训练中，常用的损失函数有哪些，应该如何进行选择
-------------------------------------------------------------------
一、常用的损失函数
交叉熵损失（Cross-Entropy Loss）
应用场景：广泛应用于分类问题，包括多分类和二分类。在大语言模型中，交叉熵损失是衡量模型预测的概率分布与真实标签概率分布之间差异的一种有效方式。
特点：交叉熵损失能够很好地处理概率分布之间的比较，对于分类问题尤其有效。它能够反映出模型预测的准确性，并指导模型在训练过程中不断优化。
负对数似然损失（Negative Log-Likelihood Loss）
实质：负对数似然损失实际上是交叉熵损失在分类问题中的一个特例。当模型输出的是概率分布时，负对数似然损失通过计算真实标签对应的概率值的负对数来评估模型的好坏。
应用场景：与交叉熵损失类似，负对数似然损失也常用于分类问题，特别是在处理多分类问题时。
序列到序列损失（Sequence-to-Sequence Loss）
应用场景：适用于生成式语言模型，如GPT系列。序列到序列损失用于评估模型生成整个文本序列的准确性。
特点：在生成式任务中，模型需要生成一个完整的序列作为输出。序列到序列损失通过计算生成序列与真实序列之间的差异来指导模型的训练。
掩码语言模型损失（Masked Language Model Loss，MLM Loss）
应用场景：主要用于自动编码式语言模型，如BERT。MLM Loss通过随机掩盖输入文本中的部分词或字符，然后要求模型预测这些被掩盖的词或字符来训练模型。
特点：MLM Loss能够促使模型更好地理解上下文信息，并学习到文本中的语义关系。它在大规模预训练语言模型中得到了广泛应用。
二、损失函数的选择依据
任务类型：
对于分类任务，如情感分析、文本分类等，可以选择交叉熵损失或负对数似然损失。
对于生成任务，如机器翻译、文本摘要等，可以选择序列到序列损失。
对于需要同时考虑上下文信息的任务，如问答系统、语言理解等，可以考虑使用掩码语言模型损失。
模型架构：
自动编码式语言模型（如BERT）通常使用MLM Loss进行预训练。
自回归式语言模型（如GPT）则通过序列到序列损失进行训练。
评估指标：
在选择损失函数时，还需要考虑评估模型的性能指标。例如，对于生成任务，除了损失函数外，还可以结合BLEU、ROUGE等评估指标来全面评估模型性能。
实验和调优：
在实际训练中，可以通过实验和调优来选择最适合当前任务和数据的损失函数。例如，可以尝试不同的损失函数组合和权重分配，以找到最优的模型性能。
综上所述，大语言模型训练中常用的损失函数包括交叉熵损失、负对数似然损失、序列到序列损失和掩码语言模型损失等。在选择损失函数时，需要根据任务类型、模型架构、评估指标以及实验调优等多个方面进行综合考虑。
```



## 2.4 超参数的设定

### 2.4.1 华为经验

首先根据默认的初始值

然后，根据曲线的陡峭，考虑是否调整学习率

如果出现欠拟合，则提高训练的轮次

如果出现过拟合，可以考虑提前停止，加载中间的checkpoint文件



### 2.4.2 待解决的问题

batch size的作用是什么？通过调整batch size，小批量梯度下降，有哪些优势，起到什么作用？

出现过拟合的其他手段，比如加入L1，L2正则化，dropout等手段，应该如何实现？































# 3. 模型训练框架选择

## 3.1 pytorch⭐

参考：https://gitee.com/iwuzhichao/sft/tree/master

```python
# 全参数微调
import torch  
from torch import nn, optim  
from torch.utils.data import DataLoader  
from transformers import GPT2LMHeadModel, GPT2Tokenizer  
from datasets import load_dataset  
  
# 假设你已经有一个预训练的GPT2模型  
model = GPT2LMHeadModel.from_pretrained('gpt2-medium')  
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')  
  
# 加载数据集  
dataset = load_dataset('your_dataset_name')  
train_dataset = dataset['train']  
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  
  
# 优化器  
optimizer = optim.Adam(model.parameters(), lr=5e-5)  
  
# 训练循环  
model.train()  
for epoch in range(3):  # 假设训练3个epoch  
    for batch in train_loader:  
        inputs, labels = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, max_length=512).input_ids, batch['labels']  
  
        optimizer.zero_grad()  
        outputs = model(inputs, labels=labels)  
        loss = outputs.loss  
        loss.backward()  
        optimizer.step()  
  
        print(f'Epoch {epoch+1}, Loss: {loss.item()}')  
  
# 保存模型  
model.save_pretrained('fine-tuned-gpt2')
```



## 3.2 transformers⭐

参考⭐：https://www.cnblogs.com/tian777/p/18026225

transformers入门：https://juejin.cn/post/7248918622692769851

聊聊transformers库——进阶-模型微调和保存：https://juejin.cn/post/7249233583146450981?searchId=2023120709411889B2EDB5F8AFC5D91A59



官方文档⭐：https://huggingface.co/docs/transformers/training



### 3.2.1 trasnformers理解

```shell
# 1. outputs.logits
代表神经网络的输出值，如果是最后一层，即是整个模型的预测值
```







## 3.2 MindSpore(华为改装的transformers)

项目地址：https://gitee.com/mindspore/mindformers



## 3.3 tensorflow



## 3.4 PaddlePaddle



## 3.5 ModelLink + Megatron-LM + AscendSpeed（分布式训练）



# 4.RLHF（强化学习人类反馈优化方法）

参考项目：https://gitee.com/charent/ChatLM-mini-Chinese

偏好方法这里介绍常见的两种：PPO和DPO，具体实现请自行搜索论文及博客。

1. PPO方法（近似偏好优化,Proximal Policy Optimization）  
   步骤1：使用微调数据集做有监督微调（SFT， Supervised Finetuning）。   
   步骤2：使用偏好数据集（一个prompt至少包含2个回复，一个想要的回复，一个不想要的回复。多个回复可以按照分数排序，最想要的分数最高）训练奖励模型（RM， Reward Model）。可使用`peft`库快速搭建Lora奖励模型。   
   步骤3：利用RM对SFT模型进行有监督PPO训练，使得模型满足偏好。   

2. 使用DPO（直接偏好优化，Direct Preference Optimization）微调（**本项目采用DPO微调方法，比较节省显存**）
   在获得SFT模型的基础上，无需训练奖励模型，取得正向回答（chosen）和负向回答（rejected）即可开始微调。微调的`chosen`文本来自原数据集[alpaca-gpt4-data-zh](https://huggingface.co/datasets/c-s-ale/alpaca-gpt4-data-zh)，拒绝文本`rejected`来自SFT微调1个epoch后的模型输出，另外两个数据集：[huozi_rlhf_data_json](https://huggingface.co/datasets/Skepsun/huozi_rlhf_data_json)和[rlhf-reward-single-round-trans_chinese](https://huggingface.co/datasets/beyond/rlhf-reward-single-round-trans_chinese)，合并后共8万条dpo数据。

   dpo数据集处理过程见`utils/dpo_data_process.py`。

DPO偏好优化数据集示例：

```json
    {
        "prompt": "为给定的产品创建一个创意标语。，输入：可重复使用的水瓶。",
        "chosen": "\"保护地球，从拥有可重复使用的水瓶开始！\"",
        "rejected": "\"让你的水瓶成为你的生活伴侣，使用可重复使用的水瓶，让你的水瓶成为你的伙伴\""
    }
```

运行偏好优化：

``` bash
#  多GPU请用accelerate launch --multi_gpu --num_processes gpu个数 dpo_train.py
python dpo_train.py
```



