

# 1. 构建问答系统思路

首先，选择合适的模型，大模型(api调用 或者 开源大模型) 或者 嵌入模型(词向量模型)

其次，知识库的搭建，主要通过输入 特定知识(pdf、txt等文档)，通过词向量模型转换为向量，存入本地向量数据库

然后，词向量数据库 和 大语言模型，是如何交互的？？？？？？？？

最后，通过提示词工程，帮助大语言模型，输出我们想要的格式 

最后的最后，封装成一个api接口进行服务！！



几个需要学习点

# 2. 词向量数据库 和 大语言模型，是如何交互的？

**通过提示词，将向量 构建成提示词，输入给大模型！**

**可以使用代码，来描述提示词为什么对模型有影响吗**

-------------------

当使用代码来描述提示词（Prompts）为什么对模型有影响时，我们可以从模型训练和推理的角度来观察。尽管我不能直接展示一个完整的训练过程，但我可以提供一个简化的概念性示例，说明提示词如何影响模型的输出。

首先，假设我们有一个简单的文本分类模型，它使用神经网络来预测输入的文本是属于哪个类别。这个模型在训练时学习到了如何从文本中提取特征并做出分类决策。

现在，我们考虑两种情况：一种情况是没有使用提示词，另一种情况是使用了提示词。

**没有使用提示词的情况**：

假设我们有一个简单的文本输入："This is a great movie." 我们直接将其输入到模型中，模型会根据其学到的特征表示来预测类别（例如，正面评价或负面评价）。

**使用了提示词的情况**：

假设我们添加了一个提示词："Rate this movie: " 到原始文本中，得到新的输入："Rate this movie: This is a great movie."

现在，我们将这个带有提示词的文本输入到模型中。由于提示词为模型提供了额外的上下文信息，模型可能会以不同的方式处理这个输入。具体来说：

1. **注意力机制**：如果模型使用了注意力机制（Attention Mechanism），它可能会更加关注提示词后面的文本，因为这些文本是用户想要评价的内容。
2. **特征提取**：模型在提取特征时，可能会因为提示词的存在而提取出与评价相关的特定特征，这些特征在没有提示词的情况下可能不那么明显。
3. **决策边界**：在训练过程中，模型会学习到带有提示词的文本与类别之间的映射关系。由于提示词为模型提供了明确的指示，这可能会导致模型的决策边界发生变化，使其更加符合带有提示词的文本数据的分布。

为了更具体地展示这一点，我们可以使用伪代码来模拟这个过程：

```python
# 假设我们有一个预训练的文本分类模型 model  
# 没有提示词的文本  
text_without_prompt = "This is a great movie."  
prediction_without_prompt = model.predict(text_without_prompt)  
  
# 带有提示词的文本  
text_with_prompt = "Rate this movie: " + text_without_prompt  
prediction_with_prompt = model.predict(text_with_prompt)  
  
# 打印预测结果以观察差异  
print("Prediction without prompt:", prediction_without_prompt)  
print("Prediction with prompt:", prediction_with_prompt)
```

请注意，上述伪代码仅用于说明目的。在实际应用中，模型的结构和训练过程会更加复杂，并且可能需要使用专门的库（如Hugging Face的Transformers）来处理提示词和文本数据。但是，这个示例应该能够清晰地说明提示词如何影响模型的预测结果。



![image-20240520111007482](https://gitee.com/fubob/note-pic/raw/master/image/image-20240520111007482.png)







# 3. GPU服务器的配置！主要是显存！

# 4. 常见向量知识库的使用：Chroma、Milvus

```
Annoy。由Spotify开发的开源向量数据库，支持高效的近似最近邻搜索和相似度计算。1
Chroma。开源的向量数据库，为开发人员和各种规模的组织提供构建基于大型语言模型的应用所需的资源。2
DingoDB。开源的分布式、实时多模向量数据库，支持多模态数据的混合存储。
Milvus。开源的向量相似性搜索引擎，专为人工智能和机器学习应用程序设计。3
Pinecone。云托管的向量数据库，支持多维向量的广泛应用，如相似性搜索、推荐系统等。
Weaviate。开源的向量数据库，用于存储和管理向量和对象，适用于多种搜索方法。
Vald。高度可扩展的、云原生的分布式向量搜索引擎，支持多种搜索算法。
Qdrant。开源的、高性能的向量搜索引擎，支持大规模数据集。
这些向量数据库各有其特点和优势，适用于不同的应用场景和需求。
例如，Annoy和Chroma更注重于高效和灵活的最近邻搜索和相似度计算；
DingoDB和Milvus则提供了更广泛的向量处理能力；
Pinecone和Weaviate则更注重于易用性和可扩展性；
Vald和Qdrant则专注于高性能的搜索和强大的可扩展性。
```

# 5. 常用的大语言模型有哪些

开源大语言模型：chartGLM，羊驼，gpt 3.5 turbo

嵌入词向量模型：wort2vec、hugging face bge small

接口调用：千问、文心、gpt-4

# 6. RAG应用的评估指标

https://www.bilibili.com/video/BV1494y1E7H9/?p=2&spm_id_from=pageDriver&vd_source=824d8f61906b474c0974b8dce18a69fd  07:33

TrueAero模块进行指标评估！

上下文相关性、基础性、答案相关性

allolens 评估生成式ai应用的标准机制

# 7. 构建个问答应用，通常需要哪几个模块

1）大语言模型的加载

2）加载本地知识库的文档

3）通过langchain的分割，对文档进行分割

4）并通过embeding模型，将转换后的文档，转换为词向量(embedding)，并存入词向量数据库

5)  加载提问，转换为词向量，输入到词向量数据库，进行相似性查询

6）通过相似性查询出的向量，结合 自己设置的prompt提示词(通过langchain.prompots设置提示词模板)，输入到大语言模型中

# 8. 面试准备

## 8.1 用的什么向量数据库？

## 8.2 用的什么向量模型？

## 8.3 用的什么大语言模型？

## 8.4 如何进行微调的？

## 8.5 服务器的资源如何？

## 8.6 Ollama和langchain有什么区别？

Ollama和LangChain在多个方面存在显著的区别。

Ollama是一个用于在本地运行大型语言模型（LLM）的开源框架。它的主要特点和功能包括：

1. 简化部署：旨在简化在Docker容器中部署LLM的过程，使管理和运行这些模型变得更加容易。
2. 捆绑模型组件：将模型权重、配置和数据捆绑到一个包中，称为Modelfile，这有助于优化设置和配置细节，包括GPU使用情况。
3. 支持多种模型：Ollama支持多种大型语言模型，如Llama 2、Code Llama、Mistral、Gemma等，并允许用户根据特定需求定制和创建自己的模型。
4. 跨平台支持：支持macOS和Linux平台，Windows平台的预览版也已发布。
5. 命令行操作：用户可以通过简单的命令行操作启动和运行大型语言模型。

此外，Ollama是一个基于Python的本地大模型运行框架，具有简单易用、高效稳定等特点。它允许开发者在本地轻松实现大模型的部署和应用，无需依赖云端资源。

而LangChain是一个新一代的AI开发框架，旨在释放大语言模型的潜能，为开发者提供便捷的开发工具和接口。它的主要能力和组件包括：

1. 将LLM模型与外部数据源进行连接。
2. 允许与LLM模型进行交互。
3. 组件：包括模型（Models）、提示模板（Prompts）等，这些组件使得提示工程流线化，进一步激发大语言模型的潜力。

综上所述，Ollama和LangChain在目标、功能和组件方面都有所不同。Ollama专注于在本地简化大型语言模型的部署和运行，而LangChain则更侧重于为大语言模型提供与外部数据源连接和交互的能力，并包含多种组件来优化大语言模型的使用。

## 8.7 使用本地知识库，对LLM模型进行微调

在Python中，使用现有的知识库文档对大模型（如BERT、GPT等）进行微调通常涉及以下步骤：

1. **准备数据**：首先，你需要将知识库文档转换为模型可以理解的格式，通常是文本数据的某种标记化形式。
2. **加载预训练模型**：选择一个适合你任务的预训练模型，并使用适当的库（如Hugging Face的Transformers库）加载它。
3. **数据预处理**：根据模型的输入要求，对数据进行必要的预处理，如填充、截断或标记化。
4. **定义微调任务**：根据你的任务（如文本分类、序列生成等）定义损失函数和优化器。
5. **微调模型**：在准备好的数据上训练模型，这通常是通过多轮迭代（epochs）完成的，每次迭代都会处理整个数据集的一部分（称为一个批次或batch）。
6. **评估模型**：在验证集上评估模型的性能，并根据需要进行调整。
7. **保存模型**：保存微调后的模型以供将来使用。

下面是一个简化的示例，说明如何使用Hugging Face的Transformers库和PyTorch来微调一个BERT模型进行文本分类任务：

```python
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments  
from torch.utils.data import Dataset, DataLoader  
  
# 假设你已经有了知识库文档，并且已经将其转换为文本分类任务所需的格式  
# 例如，每个样本都是一个文本字符串和一个对应的标签  
  
# 1. 准备数据  
class MyDataset(Dataset):  
    def __init__(self, texts, labels):  
        self.texts = texts  
        self.labels = labels  
  
    def __len__(self):  
        return len(self.texts)  
  
    def __getitem__(self, idx):  
        return self.texts[idx], self.labels[idx]  
  
# 加载数据（这里只是示例，你需要从实际的数据源加载）  
texts = [...]  # 文本列表  
labels = [...]  # 对应的标签列表  
dataset = MyDataset(texts, labels)  
  
# 2. 加载预训练模型和分词器  
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(labels)))  
  
# 3. 数据预处理（这里只是简单的标记化，你可能还需要添加填充、截断等）  
def preprocess_function(examples):  
    return tokenizer(examples['text'], padding=True, truncation=True, return_tensors='pt')  
  
encoded_dataset = dataset.map(preprocess_function, batched=True)  
  
# 4. 定义微调任务（这里只是示例，你可能需要自定义更复杂的训练逻辑）  
training_args = TrainingArguments(  
    output_dir='./results',          # 输出目录  
    num_train_epochs=3,              # 训练轮数  
    per_device_train_batch_size=16,  # 批次大小  
    warmup_steps=500,                # 预热步数  
    weight_decay=0.01,               # 权重衰减  
    logging_dir='./logs',            # 日志目录  
    logging_steps=10,                # 日志记录步数  
)  
  
# 5. 微调模型  
trainer = Trainer(  
    model=model,  
    args=training_args,  
    train_dataset=encoded_dataset,  
    compute_metrics=lambda e: {'accuracy': sum(e.predictions.argmax(dim=-1) == e.label_ids) / len(e.predictions)}  
)  
  
trainer.train()  
  
# 6. 评估模型（此步通常在实际应用中需要）  
# ...  
  
# 7. 保存模型  
# trainer.save_model('./model_save_directory')  # 如果你在训练过程中没有指定output_dir，你可以在这里保存模型
```

请注意，这只是一个简化的示例，你可能需要根据你的具体任务和数据集进行调整。此外，对于大型数据集或复杂任务，你可能还需要使用更复杂的数据预处理、训练策略、超参数调整等技术。

## 8.8 使用什么样的损失函数和优化器

### 常用的损失函数

1. **均方误差（Mean Squared Error, MSE）**：常用于回归问题，计算预测值与真实值之间差的平方的平均值。

   公式：(L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2)

2. **交叉熵损失（Cross-Entropy Loss）**：常用于分类问题，特别是当输出层使用softmax函数时

   对于二分类问题（逻辑回归），公式为：(L(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)])

### 常用的优化器

1. **随机梯度下降（Stochastic Gradient Descent, SGD）**：基础且常用的优化器，每次迭代只使用一个样本或一小批样本来计算梯度。

## 8.9 常用的机器学习算法有哪些

常用的机器学习算法有很多，包括但不限于以下几种：

1. **线性回归（Linear Regression）**：线性回归是一种用于预测数值型数据的统计方法。它试图找到最佳拟合输入变量（x）和输出变量（y）之间关系的直线，通过最小化预测值与真实值之间的误差平方和来实现。
2. **逻辑回归（Logistic Regression）**：逻辑回归是一种用于解决二分类问题的统计方法。它通过应用逻辑函数（sigmoid函数）将线性回归模型的输出转换为介于0和1之间的概率值，从而可以处理分类任务。
3. **支持向量机（Support Vector Machine, SVM）**：SVM是一种有监督学习算法，用于分类和回归分析。它试图找到一个超平面来最大化两个类别之间的边界（间隔），同时确保训练集中的数据点尽可能被正确分类。
4. **决策树（Decision Tree）**：决策树是一种基于树形结构进行决策的方法。它通过一系列的问题（节点）和可能的答案（分支）来预测结果（叶子节点）。决策树可以用于分类和回归问题。
5. **随机森林（Random Forest）**：随机森林是一种基于决策树的集成学习方法。它通过构建多个决策树，并对它们的预测结果进行平均或投票来得出最终的预测结果。随机森林通常比单个决策树具有更好的性能，因为它可以减少过拟合和偏差。
6. **朴素贝叶斯（Naive Bayes）**：朴素贝叶斯是一种基于贝叶斯定理和特征条件独立假设的分类方法。它假设输入特征的每个维度都是相互独立的，这在实际应用中可能并不总是成立，但朴素贝叶斯在许多情况下仍然能够表现出色。
7. **K-近邻（K-Nearest Neighbors, KNN）**：KNN是一种基于实例的学习方法，它根据待分类样本在特征空间中的k个最近邻的样本的类别来进行分类。KNN的性能取决于k值的选择以及距离度量的方式。
8. **梯度提升决策树（Gradient Boosting Decision Tree, GBDT）**：GBDT是一种基于决策树的集成学习方法，它通过迭代地构建多个决策树，并将它们的预测结果相加来得出最终的预测结果。在每次迭代中，GBDT都会根据之前预测结果的残差来构建新的决策树，以逐渐减小预测误差。

此外，还有一些其他的机器学习算法，如聚类算法（如K-means、层次聚类等）、神经网络（如卷积神经网络CNN、循环神经网络RNN等）、深度学习算法（如自动编码器、生成对抗网络GAN等）以及强化学习算法等。这些算法在不同的应用场景下都有其独特的优势和适用性。



## 8.10 transfoemer的原理是什么

Transformer的原理主要基于自注意力机制（Self-Attention），用于捕获序列数据中的依赖关系。Transformer模型最初由Vaswani等人在2017年的论文《Attention is All You Need》中提出，用于解决机器翻译问题。它的灵感来源于人类的阅读理解能力，即模型在处理序列的每个元素时，会同时考虑序列中的所有其他元素，从而捕捉元素之间的关系。

在Transformer模型中，自注意力机制可以看作是一个模块，输入可以是数据，也可以是网络中的隐藏层。将数据输入到该模块后，会输出对应个数的向量，并将这些结果输入至全连接网络（Fully Connected, FC），最后输出标签。这个过程可以重复多次。每个输出的向量都考虑了全局信息，即每个输出都与所有的输入相关。

Transformer模型主要由输入部分、编码器部分、解码器部分和输出部分组成。输入部分包括源文本嵌入层和位置编码层，用于将源文本中的词汇数字表示转换为向量表示，并捕捉词汇间的关系以及序列中的位置信息。编码器部分由多个编码器层堆叠而成，每个编码器层包含自注意力机制和前馈神经网络。解码器部分也由多个解码器层堆叠而成，每个解码器层包含带掩码的多头自注意力子层、多头注意力子层（编码器到解码器）和前馈全连接层。输出部分包括线性层和Softmax层，用于将解码器输出的向量转换为最终的输出维度，并将输出转换为概率分布，以便进行最终的预测。

此外，Transformer还采用了Scaled Dot-Product Attention和位置编码等技术来优化模型性能。Scaled Dot-Product Attention是对自注意力机制的一种改进，通过除以一个缩放因子来防止注意力权重过大。位置编码则用于在模型中加入序列中元素的位置信息，因为Transformer不使用递归或卷积来捕捉序列的顺序关系。

总的来说，Transformer的原理是通过自注意力机制来捕获序列数据中的依赖关系，并通过多个编码器层和解码器层的堆叠来实现对序列的深入理解和处理。

## 8.11 Self-Attention机制是什么意思

当然可以。Self-Attention（自注意力机制）是Transformer模型中的核心组件，它允许模型在处理序列中的每个位置时，都关注到序列中的其他所有位置。这种机制使得模型能够捕捉序列中长距离的依赖关系，并理解序列中不同位置之间的相关性。

以下是一个简单的例子来说明Self-Attention机制的工作原理：

假设我们有一个输入序列，其中包含4个单词：“I”, “am”, “learning”, “NLP”。在Self-Attention机制中，每个单词都会与其他单词进行“注意”交互。

1. **嵌入层（Embedding Layer）**：首先，每个单词会被转换为一个嵌入向量（embedding vector）。这些嵌入向量是模型学习到的单词的数值表示，它们捕捉了单词的语义信息。

2. **查询（Query）、键（Key）和值（Value）**：在Self-Attention机制中，每个嵌入向量都会生成三个新的向量：查询（Q）、键（K）和值（V）。这些向量是通过将原始嵌入向量与三个不同的权重矩阵相乘得到的。在实际操作中，通常会对整个序列进行这种操作，所以我们会得到一系列查询、键和值向量。

3. **计算注意力分数（Attention Scores）**：接下来，对于序列中的每个位置（即每个单词），我们都会计算一个注意力分数。这个分数是通过将当前位置的查询向量与序列中所有位置的键向量进行点积（dot product）来得到的。点积的结果衡量了当前位置与序列中其他位置之间的相似度。

   例如，当我们处理单词“learning”时，我们会计算它与“I”、“am”、“learning”和“NLP”的键向量的点积，从而得到四个注意力分数。

4. **缩放点积注意力（Scaled Dot-Product Attention）**：为了避免注意力分数过大导致softmax函数进入饱和区，我们通常会对点积结果进行缩放。这是通过将点积结果除以一个缩放因子（通常是嵌入向量维度的平方根）来实现的。

5. **应用softmax函数**：缩放后的注意力分数会被输入到softmax函数中进行归一化，从而得到一组权重。这些权重表示了序列中每个位置对当前位置的贡献程度。

6. **加权求和（Weighted Sum）**：最后，我们将序列中所有位置的值向量与对应的权重相乘，并将结果相加，得到当前位置的输出向量。这个输出向量包含了序列中所有位置的信息，但根据注意力分数的不同，不同位置的信息对当前位置的贡献也不同。

在上面的例子中，当模型处理单词“learning”时，它会根据与序列中其他单词的相似度来分配不同的注意力权重。这些权重决定了序列中哪些单词对理解“learning”更重要。通过这种方式，Self-Attention机制使模型能够捕捉序列中的长距离依赖关系，并理解不同单词之间的相关性。