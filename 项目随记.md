# 20240521

## 项目思路

#### 首先，理解问题，根据给的数据sql.db文件 + 文本(pdf、txt)资料，用自然语言回答question

```json
{"id": 0, "question": "景顺长城中短债债券C基金在20210331的季报里，前三大持仓占比的债券名称是什么?"
 , "answer": "景顺长城中短债债券C在20210331的季报中，前三大持仓占比的债券名称分别是21国开01、20农发清发01、20国信03。"} 
{"id": 1, "question": "上海华铭智能终端设备股份有限公司的首发战略配售结果如何？"
 , "answer": "上海华铭智能终端设备股份有限公司的首发战略配售具体情况并未在招股意向书中详细说明。"}
```

#### 有两部分任务，第一是rag基于文本的问答，第二是自然语言转换为sql，查询数据库



#### 1. rag基于文本的问答

将文本进行embeeding，存入向量数据库，

#### 2. 如何把自然语言转换为sql？

通过将元数据、和自然语言-->正确sql查询，进行embedding，存入向量数据库！！输入给LLM模型！！

所以第一步，都是embeeding转换！！

#### 3. 项目架构

```
## 1. 项目功能：基于文档的问答 和 自然语言转换为sql，并查询数据库

## 基于文档的问答
文档的操作：读取、切割、转换为embedding
向量数据库的操作：存入 和 读取
大模型的操作：链接、加载、输入、输出

## 自然语言转换为sql
自然语言的操作：读取、切割、转换为embedding
向量数据库的操作：存入 和 读取
大模型的操作：链接、加载、输入、输出
```





#### 问答系统的词向量数据库应该如何设计？

在设计用于问答系统的词向量数据库时，主要目标是有效地存储问题和答案的文本内容、对应的词向量以及任何相关的元数据。以下是一个建议的数据库设计，包括表结构和字段：

##### 1. 问题表（Questions）

**字段**：

- **question_id** (主键): 唯一标识每个问题的ID。
- **question_text** (文本): 问题的文本内容。
- **question_vector** (二进制或文本): 问题的词向量表示。这可以存储为二进制数据（如BLOB）或转换为字符串形式（如逗号分隔的浮点数列表）。
- **created_at** (日期时间): 问题创建的时间戳。
- **updated_at** (日期时间): 问题最后更新的时间戳（如果有更新的话）。
- **status** (枚举): 问题的状态（如“活跃”、“已解决”、“已关闭”等）。
- **other_metadata** (文本/JSON): 其他与问题相关的元数据，可以存储为文本或JSON格式。

##### 2. 答案表（Answers）

**字段**：

- **answer_id** (主键): 唯一标识每个答案的ID。
- **question_id** (外键): 与该答案关联的问题的ID。这是问题表的外键。
- **answer_text** (文本): 答案的文本内容。
- **answer_vector** (二进制或文本): 答案的词向量表示。存储格式与问题向量相同。
- **created_at** (日期时间): 答案创建的时间戳。
- **updated_at** (日期时间): 答案最后更新的时间戳（如果有更新的话）。
- **score** (整数/浮点数): 答案的评分或质量指标（可选）。
- **accepted** (布尔): 指示该答案是否被接受或选为最佳答案（可选）。
- **other_metadata** (文本/JSON): 其他与答案相关的元数据。

##### 3. 用户表（Users）（可选）

如果你的系统需要跟踪用户信息，可以添加一个用户表。

**字段**：

- **user_id** (主键): 唯一标识每个用户的ID。
- **username** (文本): 用户名。
- **password_hash** (文本): 加密后的用户密码。
- **email** (文本): 用户的电子邮件地址（可选）。
- **other_user_info** (文本/JSON): 其他与用户相关的信息。

##### 4. 索引

为了提高查询性能，你应该在关键字段上创建索引，如`question_id`、`question_vector`、`answer_id`和`answer_vector`。对于词向量字段，你可能需要使用特殊的索引技术（如近似最近邻搜索索引）来支持高效的相似度搜索。



# 20240522

### 项目思路

第一步：将数据转换为向量，存入向量数据库

第二步：想提问进行向量化，找到向量数据库中相似性高的，问答，组合起来输入大模型

第三步：通过大模型，输出结果

### 如何进行分词，并存入数据库中的

当然，我可以提供一个简化的示例来说明如何使用代码对一句话进行分词，并假设将分词后的词转化为词向量，然后将这些词向量存入一个简化的“向量数据库”（这里我们使用Python的字典作为示例）。

首先，你需要一个预训练的词向量模型，如Word2Vec。但在这个示例中，为了简化，我们将模拟这个过程。

以下是使用Python的示例代码：

```python
import jieba  # 用于中文分词的库  
from collections import defaultdict  
  
# 假设我们有一个简单的“词向量”字典（实际中你会从预训练的模型中加载）  
fake_word_vectors = {  
    '你好': [0.1, 0.2, 0.3],  
    '世界': [0.4, 0.5, 0.6],  
    # ... 其他词的向量  
}  
  
# 简化的“向量数据库”，使用字典存储  
vector_database = defaultdict(list)  
  
def segment_and_store(sentence):  
    # 使用jieba进行分词  
    words = jieba.cut(sentence, cut_all=False)  
    for word in words:  
        # 假设所有词都在我们的fake_word_vectors中（实际中需要处理未登录词）  
        if word in fake_word_vectors:  
            # 将词向量存入“向量数据库”  
            vector_database[sentence].append(fake_word_vectors[word])  
  
# 示例句子  
sentence = "你好，世界！"  
segment_and_store(sentence)  
  
# 查看结果  
print(vector_database)
```

注意：

1. 上述代码使用了`jieba`库进行中文分词，如果你处理的是英文或其他语言，你可能需要使用不同的分词库，如`NLTK`。
2. `fake_word_vectors`是一个模拟的词向量字典，实际中你会从预训练的模型中加载这些词向量。
3. `vector_database`是一个简化的“向量数据库”，它使用Python的`defaultdict`来存储每个句子对应的词向量列表。在实际应用中，你可能会使用更复杂的数据库系统来存储这些向量。
4. 上述代码没有处理未登录词（即不在词向量模型中的词）。在实际应用中，你需要决定如何处理这些词，例如忽略它们、使用特殊标记（如`<UNK>`）或使用某种回退策略。

### 如何将句子转换为向量，并比较相似性

在Python中，当你有两个自然语言的句子或短语转换为词向量（如Word2Vec、FastText或GloVe生成的词向量）后，计算它们之间的相似性通常涉及以下步骤：

1. **加载预训练的词向量模型**：首先，你需要加载一个预训练的词向量模型。这可以是使用Word2Vec、FastText等算法在大型语料库上训练得到的模型。
2. **将句子转换为词向量**：对于每个句子，你需要将其分词，并将每个词转换为对应的词向量。然后，你可能需要将这些词向量组合成一个单一的句子表示（如通过平均、加权平均、TF-IDF加权或其他更复杂的方法）。
3. **计算两个句子表示之间的相似性**：最后，你可以使用各种方法来计算两个句子表示之间的相似性，如余弦相似性、欧几里得距离、曼哈顿距离等。

以下是一个使用Gensim库和预训练的Word2Vec模型计算句子相似性的示例代码：

```python
from gensim.models import KeyedVectors  
from nltk.tokenize import word_tokenize  
from sklearn.metrics.pairwise import cosine_similarity  
import numpy as np  
  
# 加载预训练的Word2Vec模型  
model = KeyedVectors.load_word2vec_format('path_to_your_pretrained_model', binary=True)  
  
# 定义两个句子  
sentence1 = "我喜欢看电影"  
sentence2 = "电影很有趣"  
  
# 对句子进行分词  
tokens1 = word_tokenize(sentence1)  
tokens2 = word_tokenize(sentence2)  
  
# 将分词后的词转换为词向量，并计算句子向量（这里使用平均词向量作为简单示例）  
vector1 = np.mean([model.wv[token] for token in tokens1 if token in model.wv], axis=0)  
vector2 = np.mean([model.wv[token] for token in tokens2 if token in model.wv], axis=0)  
  
# 确保两个句子向量有相同的维度  
assert vector1.shape == vector2.shape  
  
# 计算两个句子向量之间的余弦相似性  
similarity = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]  
  
print(f"句子1和句子2的余弦相似性为: {similarity}")
```

注意：

- 在上面的代码中，我假设你已经有了一个预训练的Word2Vec模型，并且它保存为二进制格式。你需要将`'path_to_your_pretrained_model'`替换为你的模型文件的实际路径。
- 我还使用了`nltk.tokenize.word_tokenize`来进行中文分词，但请注意，`nltk`默认是为英文设计的。对于中文分词，你可能需要使用像`jieba`这样的库。
- 在计算句子向量时，我使用了简单的平均词向量方法。这只是一个基本示例，实际上，你可以使用更复杂的方法（如加权平均、TF-IDF加权、句子嵌入模型如BERT等）来获得更准确的句子表示。
- 最后，我使用了`sklearn.metrics.pairwise.cosine_similarity`来计算两个句子向量之间的余弦相似性。这个函数返回的是一个二维数组，但因为我们只比较两个向量，所以我们只需要取第一个元素（即`[0][0]`）。

### 比较两个向量的相似性，并选择出前5的向量

在深度学习和机器学习中，比较两个向量的相似性通常涉及到计算它们之间的距离或相似度分数。常见的相似度度量包括余弦相似度、欧几里得距离（或者其平方，即欧氏距离的平方）、皮尔逊相关系数等。在这里，我将演示如何使用PyTorch来计算两个向量集合之间的余弦相似度，并选出相似性排前5的向量。

首先，我们需要定义两个向量集合（这里用矩阵表示，每一行是一个向量）。然后，我们可以使用PyTorch的`cosine_similarity`函数（注意：这个函数在某些PyTorch版本中可能不可用，你可能需要自己实现或使用其他函数如`mm`和`norm`来计算余弦相似度）。

以下是一个使用PyTorch计算余弦相似度并选出前5个最相似向量的示例代码：

```python
import torch  
  
# 假设我们有两个向量集合，分别用tensor A和B表示  
# A的shape是[m, d]，B的shape是[n, d]，其中d是向量的维度  
m, n, d = 100, 200, 128  # 示例维度，你可以根据实际情况调整  
A = torch.randn(m, d)  # 随机初始化A  
B = torch.randn(n, d)  # 随机初始化B  
  
# 计算A中每个向量与B中所有向量的余弦相似度  
# 注意：PyTorch没有直接的cosine_similarity函数，但我们可以使用mm和norm来计算  
# 首先计算A和B的归一化（单位化）  
A_norm = A / A.norm(dim=1, keepdim=True)  
B_norm = B / B.norm(dim=1, keepdim=True)  
  
# 计算余弦相似度矩阵  
# (A_norm @ B_norm.t()) 的每个元素是 A 的一个行向量与 B 的一个列向量的点积  
# 因为A_norm和B_norm都是单位向量，点积就是余弦相似度  
cosine_similarity_matrix = A_norm @ B_norm.t()  
  
# 假设我们想要找到A中每个向量在B中最相似的5个向量  
# 对余弦相似度矩阵的每一行进行排序，并取出前5个最大的索引  
# 注意：这里我们假设我们只关心A中每个向量与B中最相似的向量，而不是所有可能的向量对  
top_indices = cosine_similarity_matrix.topk(k=5, dim=1, largest=True, sorted=True).indices  
  
# 现在top_indices包含了A中每个向量在B中最相似的5个向量的索引  
# 你可以根据需要进一步处理这些索引，例如提取对应的B中的向量  
  
# 例如，提取A中第一个向量在B中最相似的5个向量  
most_similar_in_B_for_first_in_A = B[top_indices[0]]  
print(most_similar_in_B_for_first_in_A)
```

请注意，这个示例代码计算了A中每个向量与B中所有向量的余弦相似度，并找到了A中每个向量在B中最相似的5个向量。如果你只关心A中的一个特定向量与B中所有向量的相似度，你可以直接对该向量进行相似度计算并排序。

### 如何将一句话转换为向量 后，存入词向量数据库

将一句话转换为向量并存入词向量数据库通常涉及几个步骤。这里我将提供一个基本的流程，包括如何生成词向量（通常使用预训练的词嵌入模型如Word2Vec、GloVe等）以及如何将它们存储到数据库中。

##### 1. 准备预训练的词向量模型

首先，你需要一个预训练好的词向量模型。这些模型已经为大量的词汇计算了向量表示。你可以从开源资源中下载，例如Google的Word2Vec模型或Stanford的GloVe模型。

##### 2. 加载词向量模型

使用适当的库（如`gensim`对于Word2Vec）加载预训练的词向量模型。

```python
from gensim.models import KeyedVectors  
  
# 加载模型  
model = KeyedVectors.load_word2vec_format('path_to_your_pretrained_model', binary=True)  # 根据你的模型调整参数
```

##### 3. 将句子转换为向量

句子本身不能直接转换为一个单一的向量，但你可以使用多种方法来表示一个句子，例如：

- **平均词向量**：计算句子中所有词的向量的平均值。
- **TF-IDF加权词向量**：使用TF-IDF为每个词分配权重，然后计算加权平均值。
- **句子嵌入**（如Doc2Vec）：使用专门为句子设计的嵌入模型。

这里，我们使用平均词向量的简单示例：

```python
def sentence_to_vector(sentence, model):  
    vectors = [model.wv.get_vector(word) for word in sentence.split() if word in model.wv]  
    if not vectors:  
        return None  # 或返回一个默认的向量，如零向量  
    return np.mean(vectors, axis=0)  
  
sentence = "这是一个示例句子"  
vector = sentence_to_vector(sentence, model)
```

##### 4. 存入词向量数据库

接下来，你可以将生成的句子向量存储到数据库中。这取决于你选择的数据库系统，但以下是一个使用SQLite的简单示例：

```python
import sqlite3  
  
# 连接到SQLite数据库（如果数据库不存在，它将被创建）  
conn = sqlite3.connect('sentence_vectors.db')  
cursor = conn.cursor()  
  
# 创建一个表来存储句子和它们的向量（这里我们假设向量被转换为字符串格式）  
cursor.execute('''CREATE TABLE IF NOT EXISTS sentence_vectors  
                 (sentence TEXT, vector TEXT)''')  
  
# 将句子和它的向量插入到表中（注意：这里你需要将向量转换为字符串格式，例如使用numpy的tostring方法）  
vector_str = ' '.join(map(str, vector.tolist()))  # 将numpy数组转换为字符串  
cursor.execute("INSERT INTO sentence_vectors (sentence, vector) VALUES (?, ?)", (sentence, vector_str))  
  
# 提交事务并关闭连接  
conn.commit()  
conn.close()
```

**注意**：在实际应用中，将浮点数数组直接存储为字符串可能不是最有效的方法，特别是对于大型数据集。你可能需要考虑使用二进制格式或专门的数据库系统（如PostgreSQL的Array类型或MongoDB的BSON格式）来更有效地存储和查询向量数据。



### 所谓token到底是什么意思以及[CLS] [SEP]的含义

在自然语言处理（NLP）和计算机科学中，特别是在处理文本数据时，"token" 是一个常用的术语，它指代文本中的最小单位或元素。这些单位或元素可以是字符、单词、标点符号，或者是由特殊算法（如分词器或词干提取器）生成的更复杂的结构。

在NLP的上下文中，"tokenization" 是将连续的原始文本切分成多个token的过程。这个过程通常包括移除空白字符、标点符号分隔、以及将文本切分成单词或符号。对于某些语言，如中文，分词可能更复杂，因为单词之间没有明显的分隔符。

以下是一些tokenization的例子：

1. 英文句子："The quick brown fox jumps over the lazy dog" 可以被切分为以下token：["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]
2. 中文句子："我爱自然语言处理" 可以被切分为以下token（注意中文分词可能因算法而异）：["我", "爱", "自然", "语言", "处理"]
3. 字符级tokenization：任何文本都可以被切分为单个字符。例如，"Hello" 可以被切分为 ["H", "e", "l", "l", "o"]

在深度学习模型中，如BERT，token通常用于表示文本中的最小单位。BERT模型中的每个token都会被映射到一个高维向量（也称为嵌入向量或embedding），这些向量可以被模型用于理解文本的含义。

此外，一些特殊的token，如`[CLS]`（分类token）和`[SEP]`（分隔符token），在BERT等模型中用于特定的目的。例如，`[CLS]` token的输出通常用于表示整个输入序列的含义，而`[SEP]` token用于分隔不同的输入序列（如句子对）。

### 张量的切片选择

假设我们有一个三维的Tensor `x`，其形状（shape）为 `(a, b, c)`。

- 第一个冒号 `:` 表示第一个维度（索引为0的维度）上的所有元素都被选取。也就是说，对于所有可能的索引 `i`（其中 `0 <= i < a`），`x[i, 0, :]` 都会被选取。
- `0` 表示第二个维度（索引为1的维度）上的索引为0的元素被选取。
- 最后一个冒号 `:` 同样表示第三个维度（索引为2的维度）上的所有元素都被选取。对于所有可能的索引 `j`（其中 `0 <= j < c`），`x[i, 0, j]` 都会被选取。

因此，`x[:, 0, :]` 的结果是一个形状为 `(a, c)` 的二维Tensor，其中包含了原Tensor `x` 中第二个维度索引为0的所有切片。

```python
last_hidden_states1 = outputs1.last_hidden_state[:, 0, :] ## 第一维度去所有值，第二个维度取索引为0的元素，第三个维度选取所有
```

### 向量数据库

Chroma、Milvus(支持近邻搜索)、es

### 创建两个张量，比较相似性

在Python中，我们通常使用TensorFlow或PyTorch这样的库来创建和处理张量（tensors）。以下是如何在TensorFlow和PyTorch中分别创建两个张量，并计算它们之间相似性的示例。

##### 在TensorFlow中：

```python
import tensorflow as tf  
  
# 创建两个张量  
tensor1 = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  
tensor2 = tf.constant([[1.0, 2.0, 3.1], [4.1, 5.0, 6.0]])  
  
# 计算两个张量之间的相似性，例如使用余弦相似度  
# 注意：TensorFlow本身没有直接的余弦相似度函数，但我们可以自己实现  
# 这里我们计算两个向量的余弦相似度作为示例（仅当张量形状为[D]时）  
# 对于矩阵，你可能需要计算每行或每列之间的相似度  
  
# 假设我们计算第一行之间的余弦相似度  
vector1 = tensor1[0]  
vector2 = tensor2[0]  
  
# 标准化向量  
vector1_norm = tf.math.l2_normalize(vector1, axis=0)  
vector2_norm = tf.math.l2_normalize(vector2, axis=0)  
  
# 计算余弦相似度  
cosine_similarity = tf.reduce_sum(tf.multiply(vector1_norm, vector2_norm))  
  
print(cosine_similarity)
```

##### 在PyTorch中：

```python
import torch  
  
# 创建两个张量  
tensor1 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  
tensor2 = torch.tensor([[1.0, 2.0, 3.1], [4.1, 5.0, 6.0]])  
  
# 计算两个张量之间的相似性，例如使用余弦相似度  
# PyTorch提供了一个直接的余弦相似度函数  
  
# 计算所有向量对之间的余弦相似度（这将返回一个2x2的矩阵）  
cosine_similarity = torch.nn.functional.cosine_similarity(tensor1.unsqueeze(0), tensor2.unsqueeze(0), dim=2)  
  
# 如果你只关心第一行之间的余弦相似度，可以索引结果  
cosine_similarity_first_row = cosine_similarity[0, 0]  
  
print(cosine_similarity_first_row)
```

请注意，余弦相似度是计算两个向量之间相似度的一种方法，但它仅适用于具有相同长度的向量。如果你的张量是二维的（即矩阵），并且你想计算行与行或列与列之间的相似度，你可能需要迭代这些行或列，或者将问题重塑为向量之间的相似度计算。

# 20240523

### 评估指标

em: 答案的匹配程度

f1: 综合了模型的准确率和召回率的评估指标

### LSTM是什么

### attention是什么

可以举例说明一下自注意力机制吗？

****

自注意力机制（Self-Attention）是一种在深度学习模型中应用的机制，尤其在处理序列数据（如文本或时间序列）时非常有效。这种机制允许输入序列的每个元素都与序列中的其他元素进行比较，以计算序列的表示。下面通过几个例子来解释自注意力机制：

1. 文本处理中的自注意力机制：

假设我们有一个句子：“我喜欢深度学习，因为它很有趣。” 在自注意力机制中，模型会处理这个句子中的每个单词（例如，“我”，“喜欢”，“深度学习”等），并为它们生成查询（Query）、键（Key）和值（Value）的向量表示。

- 查询（Query）：表示当前正在处理的单词，用于与序列中的其他单词进行比较。
- 键（Key）：表示序列中的其他单词，用于与查询进行比较以确定相似度。
- 值（Value）：与键相关联的向量，表示该单词的某种特征或属性。

在处理“我喜欢深度学习”这个短语时，模型会首先处理“我”这个词，并将其作为查询。然后，模型会将“我”的查询与句子中所有单词（包括“我”本身）的键进行比较，以计算相似度分数。这些相似度分数将用于确定在生成“我”这个词的上下文表示时，应该给予句子中其他单词多大的权重。最后，模型将使用这些权重和相应的值来生成“我”这个词的上下文表示。

类似地，模型将继续处理句子中的其他单词，并为它们生成上下文表示。通过这种方式，自注意力机制允许模型捕捉句子中单词之间的复杂依赖关系，并生成更加准确的句子表示。

1. 图像处理中的自注意力机制：

在图像处理中，自注意力机制可以用于捕捉图像中不同区域之间的依赖关系。例如，在图像分类任务中，模型可能需要关注图像中的特定区域（如人脸、车辆等）以进行准确的分类。通过使用自注意力机制，模型可以为图像中的每个区域生成查询、键和值的表示，并计算它们之间的相似度分数。这些相似度分数将用于确定在生成图像表示时应该给予不同区域多大的权重。通过这种方式，自注意力机制允许模型聚焦于图像中的关键区域，并忽略那些对分类任务不太重要的区域。

总的来说，自注意力机制是一种强大的机制，可以捕捉输入数据中的复杂依赖关系，并在各种深度学习任务中取得显著的性能提升。

### SoftMax是什么

所以，我总结一下，softmax的用处，输入整句或者段落转换后的词向量，加上经过类似LSTM，context encoding编码后的隐藏向量，输出的一系列的单词，以及对应的概率，对吗

---

您的总结基本上是正确的，但有一些小细节需要调整。以下是更准确的描述：

Softmax的用处在于将模型的输出分数（logits或scores）转换为概率分布，这些分数通常是由神经网络（如LSTM）基于输入的文本（整句或段落）计算得出的。在这个过程中，文本首先会被转换为词向量序列，然后这些词向量会被输入到神经网络中，经过编码（如LSTM的context encoding）生成隐藏状态向量。这些隐藏状态向量包含了文本序列的上下文信息。

在神经网络的最后一层（通常是输出层），会基于这些隐藏状态向量计算出一组分数，每个分数对应词汇表中的一个单词。然后，softmax函数会被应用于这些分数，将它们转换为概率分布，即每个单词的预测概率。这个概率分布表示了在给定上下文的情况下，模型认为接下来最可能出现的是哪些单词，以及它们各自的出现概率。

因此，softmax的输出不是“一系列的单词”，而是一组单词的预测概率。我们通常会选择概率最高的单词作为预测结果，但也可以根据需要选择概率排名前几的单词作为候选。

**总结来说，softmax的用处是将模型的输出分数转换为概率分布，这些分数是基于输入的文本序列经过神经网络编码后计算得出的。输出的概率分布表示了模型对于接下来可能出现的单词的预测。**

### 优化器是什么



### SGD是什么

SGD是一种优化算法，指在找出模型的最佳参数。

SGD的基本原理是通过不断调整模型参数来最小化损失函数。在每次迭代中，SGD从训练数据中随机选择一个小批量（mini-batch）的样本来计算梯度，并使用梯度的相反方向来更新模型参数。这种随机选择样本的方式使得SGD在计算上更为高效，尤其适用于大规模数据集。

tensorBoard

# 20240527

#### 重排模型

在rag应用中，你知道有哪些开源的重排模型吗

****

在RAG（Retrieval Augmented Generation，检索增强生成）应用中，确实存在一些开源的重排模型，这些模型能够提升初步检索结果的准确性和相关性。以下是一些知名的开源重排模型：

1. 基于Cross Encoder的重排模型：
   - Cross Encoder是一种常见的重排模型，它能够对初步检索到的文档进行二次排序。通过训练一个Cross Encoder模型，可以根据文档与用户查询的相关性得分来重新排列文档的顺序。
   - 在一些RAG系统中，如LangChain和LlamaIndex，可能会使用基于Cross Encoder的重排模型来优化检索结果。
2. Decoder-Only的LLM Reranker：
   - 这是今年3月份BGE发布的一种新型重排模型，它使用decoder-only的大模型（如GPT系列）来对段落进行重新排序。通过输入用户查询和初步检索到的文档段落，LLM Reranker能够生成一个重新排序的文档列表。
   - 这种模型在实验中表现出了很好的效果，并且具有非常有前景的潜力。
3. 自定义重排模型：
   - 除了上述提到的开源重排模型外，开发者还可以根据自己的需求和数据集来训练自定义的重排模型。这通常涉及使用深度学习框架（如TensorFlow或PyTorch）和自然语言处理库（如Transformers）来构建和训练模型。
   - 在自定义重排模型时，开发者可以探索不同的网络结构、损失函数和优化策略，以找到最适合自己应用场景的模型。
4. 评估指标：
   - 在评估重排模型的性能时，通常会使用命中率（Hit Rate）和平均倒数排名（MRR）等指标。这些指标能够量化模型在检索和重排任务上的表现，并为模型选择和优化提供指导。
5. 开源工具与框架：
   - 对于RAG应用的开发者来说，还有一些开源的工具和框架可以帮助他们构建和优化重排模型。例如，LangChain和LlamaIndex提供了丰富的RAG应用构建工具和接口，支持集成不同的排序和重排模型。
   - 开发者可以利用这些工具和框架来快速搭建自己的RAG系统，并进行实验和迭代优化。

需要注意的是，上述开源重排模型可能需要根据具体的应用场景和数据集进行适配和调整。因此，在使用这些模型时，建议仔细阅读相关文档和教程，并结合自己的实际情况进行选择和优化。

****

![image-20240527171036807](https://gitee.com/fubob/note-pic/raw/master/image/image-20240527171036807.png)

![image-20240527171056582](https://gitee.com/fubob/note-pic/raw/master/image/image-20240527171056582.png)

****

使用了重排技术的向量数据库：pinecone

#### 嵌入模型

![image-20240527170151490](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240527170151490.png)

#### 如何使用tensorbroadx，观察loss？

#### conda库的快速迁移

```python
import subprocess


def export_pip_packages_to_txt(output_file):
    # 使用pip list命令获取已安装包列表
    result = subprocess.run(['pip', 'list', '--format=freeze'], stdout=subprocess.PIPE, text=True)

    # 检查命令是否成功执行
    if result.returncode != 0:
        print(f"Error executing pip list: {result.stderr}")
        return

        # 将输出写入到文件
    with open(output_file, 'w') as f:
        f.write(result.stdout)

    # 调用函数并指定输出文件的名称


export_pip_packages_to_txt('pip_packages.txt')
```

```bash
pip install -r pip_packages.txt  ## 安装
```

#### 天池实验室环境搭建

##### 1. 创建虚拟环境

```
conda init bash
source /root/miniconda3/etc/profile.d/conda.sh
```

##### 2. 创建虚拟环境

下载相关库，参考：**conda库的快速迁移**

##### 3. 下载LLM模型

```
git clone https://opencsg.com/models/shareAI/llama3-Chinese-chat-8b.git --depth 1
cd llama3-Chinese-chat-8b
git lfs pull
```

##### 4.git lfs安装

```
wget https://github.com/git-lfs/git-lfs/releases/download/v2.9.0/git-lfs-linux-amd64-v2.9.0.tar.gz
tar -xf git-lfs-linux-amd64-v2.9.0.tar.gz
chmod 755 install.sh
sudo ./install.sh
git lfs --version
```

# 20240528

#### 项目思路

1. 调用本地开源大模型，进行问答
2. 将检索后的文本上下文，输入模型进行输出
3. 解析pdf文档，存入向量数据库
4. 进行近似搜索 + 二次排序，输出文本上下文

#### langchain组件

![image-20240528102642031](https://gitee.com/fubob/note-pic/raw/master/image/image-20240528102642031.png)

![image-20240528102711325](https://gitee.com/fubob/note-pic/raw/master/image/image-20240528102711325.png)

#### 提示词(prompt engineering)

##### sql提示词

![image-20240528105534544](https://gitee.com/fubob/note-pic/raw/master/image/image-20240528105534544.png)